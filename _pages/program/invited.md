---
title: Invited Speakers
hide_title: true
layout: single
permalink: /program/invited/
sidebar:
  nav: program
toc: true
toc_only: false
toc_sticky: true
toc_icon: "cog" 
---
<h1>Invited Speakers</h1>

<h2><a href="https://www.egrefen.com/" target="_blank">Edward Grefenstette</a></h2><br/>
<img src="/assets/images/edward.jpg" alt="Edward Grefenstette">

<b>Title:</b> Going beyond the benefits of scale by reasoning about data <br/>

<b>Date:</b> 2 May 9:30-10:30<br/>

<b>Abstract:</b> Transformer-based Large Language Models (LLMs) have taken NLP—and the world—by storm. This inflection point in our field marks a shift from focussing on domain-specific neural architecture design and the development of novel optimization techniques and objectives to a renewed focus on the scaling of model size and of the amount of data ingested during training. This paradigm shift yields surprising and delightful applications of LLMs, such as open-ended conversation, code understanding and synthesis, some degree of tool-use, and some zero-shot instruction-following capabilities. In this talk, I outline and lightly speculate on the mechanisms and properties which enable these diverse applications, and posit that the training regimen which enables these capabilities points to a further shift, namely one where we go from focussing on scale, to focussing on reasoning about what data to train on. I will briefly discuss recent advances in open-ended learning in Reinforcement Learning, and how some of the concepts at play in that work may inspire or directly apply to the development of novel ways of reasoning about data in supervised learning, in particular in areas pertaining to LLMs.<br/>

<b>Speaker Bio:</b> Ed  Grefenstette is the Head of Machine Learning at Cohere,  a provider of cutting-edge NLP models that's solving all kinds of language problems; including text summarization, composition, classification and more. In addition, Ed is an Honorary  Professor at UCL.   Ed's  previous industry experience comprises Facebook  AI Research (FAIR), DeepMind,  and Dark  Blue Labs, where he was the CTO (acquired  by Google in 2014). Prior to this, Ed worked at the University of Oxford's Department of Computer Science, and was a Fulford Junior Research Fellow at Somerville College, whilst also lecturing students at Hertford College taking Oxford's new computer science  and philosophy course.  Ed's  research interests span several topics, including natural language and generation, machine reasoning, open ended learning, and meta-learning.<br/>


<h2><a href="http://www.kevinmunger.com/" target="_blank">Kevin Munger</a></h2><br/>
<b>Date:</b> 3 May 14:45-15:45<br/>

<h2><a href="https://web.eecs.umich.edu/~chaijy/" target="_blank">Joyce Chai</a></h2><br/>
<img src="/assets/images/joycechai.jpg" alt="Joyce Chai">

<b>Date:</b> 4 May 14:15-15:15<br/>

<b>Speaker Bio:</b> Joyce Chai is a Professor in the Department of Electrical Engineering and Computer Science at the University of Michigan. Before joining UM in 2019, she was a Professor of Computer Science and Engineering at Michigan State University. She holds a Ph.D. in Computer Science from Duke University. Her research interests span from natural language processing and embodied AI to human-AI collaboration. She is fascinated by how experience with the world and how social pragmatics shape language learning and language use; and is excited about developing language technology that is sensorimotor grounded, pragmatically rich, and cognitively motivated. Her current work explores the intersection between language, perception, and action to enable situated communication with embodied agents. She served on the executive board of NAACL and as Program Co-Chair for multiple conferences – most recently ACL 2020. She is a recipient of the National Science Foundation Career Award and has received several paper awards with her students (e.g., the Best Long Paper Award at ACL 2010 and an Outstanding Paper Award at EMNLP 2021). She is a Fellow of ACL.<br/>
